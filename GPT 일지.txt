GPT 일지
.gitignore 파일 의미
git 버전 관리에서 제외할 목록을 지정하는 파일

가상환경에서 세팅할거임.

requiremnts.txt를 작성 적용 하기 

requiremnts.txt 있는 버전 전부 한번에 다운받기
pip install -r requirements.txt

jupyter 사용해보기
확장자 파일.ipynb
select kernel = 어떤 환경에서 작업할지 select interpreter 개념
cell 공간을 분리해서 가동 시킬 수있다.
굳이 print를 안하고 한번더 해당 변수를 적으면 값을 보여준다.
같은 셀이아니여도 변수 값은 연결 되어있다.
- ESC: "셀 편집 모드"에서 "셀 선택 모드"로 전환
- ENTER: 선택된 셀 편집하기(커서 생성)
- CTRL+ENTER: 현재 셀 코드 실행하기
- SHIFT+ENTER: 셀 실행한 뒤 새로운 셀 생성하기
- D D: 선택된 셀 삭제하기
- A / B: 위로(A) 혹은 아래로(B) 새로운 셀 생성하기

llm과 chatmodel의 차이점
llm은 text-davinchi 를 쓰고 
chatmodel gpt-3.5 turbo를 쓴다. 
가격측면에서 gpt-3.5 turbo 1/10 더 싸다 
그러므로 특별한 이유가 없다면 chatmodel를 쓰는 게 좋다.

chat= ChatOpenAI(
    OPENAI_API_KEY= sk#####
)
이런식으로도 지정해줄 수 있다.
chatmodel 추천!

chat = chatOpenAI(temperature = 0.1)
from langchain.schema import HumanMessage, AIMessage, SystemMessage
humanmessage= 우리가쓰는거
aimessage= ai에게 보내지는것
systemmessage= llm 설정들을 제공하기 위한 메세지

ex)
message=[
    SystemMessage(
        content="당신은 geographic 전무가 입니다. 당신의 대답은 오로직 한글로 만합니다."
    ),
    AIMessage(content="my name is justin"),
    HumanMessage(content="한국 과 미국은 얼마나 먼가요 그리고 당신의 이름은 뭔가요?")
]
Systemesage에서 한글={language} 바꿔작성 가능하다.

프롬프트 작성
ChatPromptTemplate는 메세지 form 를 message로 만든다.
promptTemplate는 그냥 string을 이용해서 만든다.


template= ChatPromptTemplate.from_messages/메시지 폼작성처럼하면된다.
prompt=template.format_messages/괄호에 들어있는 값 작성
chat.predict_messages(prompt)(값출력)

outputparser
써야하는 이유는 LLM의 응답을 변형해야 할 때가 있기 때문이다.(db,dict,tuple)
outputparser 꼭 메소드를 작성해줘야한다.
파이썬 문법 strip()는 앞뒤 문장에 공백을 전부 제거해준다.
체인 form
chain = template | chat | CommaOutputParser()
chain.invoke({
    "max_items": 10,}) 입력값 주기

from langchain.callbacks import StreamingStdOutCallbackHandler
ChatOpenAI(stream = True,callbacks=[])
streaming=모델이  response이 생성되는걸 볼 수 있게 함
callbacks= 응답의 진행을 볼 수있다.

langchain module
model I/O input output의미 예)prompts, language models, output parsers

retrieval 외부 데이터로 접근하여 이를 어떻게 모델에 제공하는것

chains  template | chat | CommaOutputParser()

agents 독립적으로 ai가 작동하도록 만들 수 있게 해주는 agents가 있어

momory 기억하게 하는것

callbacks intermediate 즉 중간에 어떤일을 하는 지 보여줌.

FewShotPromptTemplate 내가원하는 비슷한 유형의 대답을 해주길 원한다면 사용하기용이 할 것 같다.
prompt template를 디스크에 저장하고 load 할 수 있다.
Fewshot은 모델에게 예제를 준다는 뜻과 같다.(cs고객센터)
Fewshot_Template form
example_template ="""
    Human : {question}
    AI : {answer}
"""
prompt= FewShotPromptTemplate(
    example_prompt= example_prompt,
    examples= examples,
    suffix= "Human : what do you know about {country}?",
    input_variables= ["country"]
)
suffix 형식화 된 모든 예제 마지막에 나오는 내용

github에서 파일을 검색하고 볼때 오른쪽에 파일보는 것을 view 파일로 바꿔서 보면 쉽게 볼 수 있다.

FewShotChatMessagePromptTemplate 우리가 대화하는 형식으로 적합하다. 더직관적이다.

example_prompt = ChatPromptTemplate.from_messages([
    ("human", "What do you know about {country}?"),
    ("ai","{answer}")
])
example_prompt= FewShotChatMessagePromptTemplate(
    example_prompt= example_prompt,
    examples= examples,
)

final_prompt = ChatPromptTemplate.from_messages([
    ("system", "you are a geography expert,you give short answer."),
    example_prompt,
    ("human","What do you know about {country}?"),
])

프롬프트가 많을 수록 돈을 많이 낸다.

example_selector를 사용하고 싶으면 FewshotpromptTemplate 와 promptTemplate 를 사용해야한다.

randomexample_selector	
chat,examples,class randomexampleselecotr,example_prompt,example_selector,prompt

프롬프트 가져와서 쓰는 방법
json 파일과 yaml 파일작성
joson form
{
    "_type": "prompt",
    "template": "What is the capital of {country}",
    "input_variables": [
        "country"
    ]
}
yaml form
_type: "prompt"
template: "What is the capital of {country}"
input_variables: ["country"]

 prompt들의 메모리를 다 모으는 방법